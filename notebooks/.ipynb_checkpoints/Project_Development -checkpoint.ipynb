{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Product Extraction and Consolidation\n",
    "\n",
    "Problem state : our task was to extract various product Entities \n",
    "\n",
    "Brand\n",
    "Catalog#\n",
    "City\n",
    "Country\n",
    "Dilution\n",
    "Product\n",
    "Purity\n",
    "Quantity\n",
    "State\n",
    "\n",
    "\n",
    "Example:\n",
    "goat serum blocking solution --:Shanghai Haoran Biological Technology,  \n",
    "DDX3 antibodies : Abcam\n",
    "\n",
    "\n",
    "Limitations with our current pipeline:\n",
    "\n",
    "Brands were incomplete\n",
    "Product extraction was incomplete \n",
    "\n",
    "Our pipeline up and running in production \n",
    "\n",
    "we just want to consolidate\n",
    "1st Task: consolidation not only wrt semantics but they must be close or grouped together by their properties or similarity \n",
    "\n",
    "subtask performed: \n",
    "Metamap https://lhncbc.nlm.nih.gov/ii/tools/MetaMap.html one tool to get UMLS https://www.nlm.nih.gov/research/umls/index.html\n",
    "\n",
    "|product  | matched candidate| sem_type | sem_group|\n",
    "|-- | --| -- | --|\n",
    "|strain Mycobacterium smegmatis mc2 4517| ['STRAIN', 'MYCOBACTERIUM SMEGMATIS'] | ['orgm', 'bact'] | LIVB|\n",
    "\n",
    "\n",
    "Analysis and Observation\n",
    "we have observed consolidation was good but the product extraction has to be more precise\n",
    "Brand and Product names are brokens or incorrect \n",
    "\n",
    "Product data was inconsistent \n",
    "eg:\n",
    "software  version 11\n",
    "n  7AAD, \n",
    "\n",
    "\n",
    "Previous Model used\n",
    "1) Existing Model used was scibert \n",
    "\n",
    "Data size : 2000 samples \n",
    "Training : 1700 sample\n",
    "Dev: 200 \n",
    "Test : 100\n",
    "\n",
    "--- Accuracy 66%\n",
    "\n",
    "\n",
    "Task for optimization of Product extraction\n",
    "\n",
    "1) To Increase the variational data size and train existing model\n",
    "---200 data samples \n",
    "\n",
    "2) Test Existing Model with appending to existing Data\n",
    "--- Accuracy 66%\n",
    "\n",
    "3) Research and Found state of art architechture model : KebioLM\n",
    "--- Accuracy 72%\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained Models used for Fine Tune\n",
    "\n",
    "1) Scibert: 1.14M papers from Semantic Scholar \n",
    "2) BioBERT\n",
    "3) Biomed_roberta_base\n",
    "4) ChemBERTa_zinc\n",
    "5) wwm\n",
    "\n",
    "Later Model used \n",
    "BioM- Electra-Base/Large-Discriminator/Generator\\\n",
    "\n",
    "\n",
    "electra- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent\n",
    "Goal is to minimize the sum of squared residuals\n",
    "“Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the lowest point of that function.”\n",
    "\n",
    "\n",
    "The steps of the algorithm are\n",
    "1. Find the slope of the objective function with respect to each parameter/feature. In other words, compute the gradient of the function.\n",
    "\n",
    "2. Pick a random initial value for the parameters. (To clarify, in the parabola example, differentiate “y” with respect to “x”. If we had more features like x1, x2 etc., we take the partial derivative of “y” with respect to each of the features.)\n",
    "3. Update the gradient function by plugging in the parameter values.\n",
    "4. Calculate the step sizes for each feature as : step size = gradient * learning rate.\n",
    "5. Calculate the new parameters as : new params = old params -step size\n",
    "6. Repeat steps 3 to 5 until gradient is almost 0.\n",
    "\n",
    "\n",
    "https://www.mygreatlearning.com/blog/gradient-descent/\n",
    "The primary task of Gradient Descent is to find the minimum of this cost function.\n",
    "To find minimum of this cost function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADAM\n",
    "https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
    "\n",
    "Adam Configuration Parameters\n",
    "alpha. Also referred to as the learning rate or step size. The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training\n",
    "beta1. The exponential decay rate for the first moment estimates (e.g. 0.9).\n",
    "beta2. The exponential decay rate for the second-moment estimates (e.g. 0.999). This value should be set close to 1.0 on problems with a sparse gradient (e.g. NLP and computer vision problems).\n",
    "epsilon. Is a very small number to prevent any division by zero in the implementation (e.g. 10E-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patbase Extraction \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition using Transformers\n",
    "\n",
    "model link https://keras.io/examples/nlp/ner_transformers/\n",
    "\n",
    "dataset:https://www.clips.uantwerpen.be/conll2003/ner/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
