Transformers Need and why they are efficient
Neural networks usually process language by generating fixed- or variable-length vector-space representations.
After starting with representations of individual words or even pieces of words, they aggregate information
from surrounding words to determine the meaning of a given bit of language in context

RNN works to take information

[refer here in detail explanation](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)


### Transformer blocks


#### Encoder Block

1) Self Attention Block: Good at modeling dependency between
different parts of sentence.Dependency can be syntatic or coreferences.

    https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a

    Need of Multi head Attention?
    Attention is not computed once as there is no single attention pattern helps in some
    cases, but there could be multiple attention patterns computed at same time.

2) Feed Forward Neural Network Block

#### Decoder Block
1) Self Attention Block

2) Encoder-Decoder Attention Block

3) Feed Forward Neural Network Block



Why Transformer has more strength
 1) Parallelism: All operations in parallel i.e Input sequence
 is passed in parallel


Need of Positional Embedding

positional encoder: vector that gives context based on position of input word
in sentence
Permutation Invariant:If change order of words
the transformer will be invariant to those changes
ie.Exact same output if we change the order of words

Need of Masked multi headed attention

If we are going to use all word in output it will no learning
First output element of decoder only had access to first element
similarly second element only had access to first and second element input to decoder_inputs


