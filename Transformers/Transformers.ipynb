{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Modeling \n",
    "\n",
    "https://lena-voita.github.io/nlp_course/language_modeling.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why to go for transformer\n",
    "\n",
    "Lets dive into basics of RNN and sequence to sequence model \n",
    "\n",
    "## RNN \n",
    "The RNN is the same as the n-gram model, except that the output of the current input will depend on the output of all the previous computations\n",
    "\n",
    "- It allows the network to keep a history of previously learned parameters and use it to predict the following output, which overcomes the problem of word order and removes the computation cost, as we’ll just pass the words individually on our model.\n",
    "\n",
    "Types of RNN\n",
    "\n",
    "1. One to One \n",
    "2. One to many: Music generation\n",
    "3. Many to One: Sentiment analysis\n",
    "4. Many to many: Named-entity Recognition, Machine Translation. \n",
    "\n",
    "\n",
    "**Its Variants**\n",
    "1. LSTM\n",
    "2. GRU\n",
    "\n",
    "**Problems**\n",
    "\n",
    "1. Vanishing or exploding gradients problem. \n",
    "2. We can’t parallelize the computations, as the output depends on previous calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Modeling\n",
    "\n",
    "In Sequence modeling how probable the sequence is or ability to estimate the likeliness of the sequence.\n",
    "\n",
    "Sequential data has three properties:\n",
    "\n",
    "1. Elements in the sequence can repeat\n",
    "2. It follows order (contextual arrangement)\n",
    "3. Length of data varies (potentially infinitely)\n",
    "\n",
    "![seqvssupervised](./Sequence-modeling.jpg)\n",
    "\n",
    "\n",
    "### Modeling Sequence \n",
    "\n",
    "<b>1. Conditional Probability</b>\n",
    "\n",
    "<b>p(xT) = p(xT | x1…., xT-1).</b>\n",
    "\n",
    "eg:\n",
    "Cryptocurrency is the next big ______\n",
    "\n",
    "\n",
    "|Target        |p(x/context)|\n",
    "|--------------|----------- |\n",
    "|Cryptocurrency|p(x1)       |\n",
    "|Cryptocurrency <b>is|p(x2/x1)|\n",
    "|Cryptocurrency is <b>the|p(x3/x2,x1)|\n",
    "|Cryptocurrency is the <b>next|p(x4/x3,x2,x1)|\n",
    "|Cryptocurrency is the next <b>big|p(x5/x4,x3,x2,x1)|\n",
    "|Cryptocurrency is the next big <b>thing|p(x6/x5,x4,x3,x2,x1)|\n",
    "\n",
    "    \n",
    "This type of approach works well with a few sentences, and captures the structure of the data very well. But when we deal with paragraphs, then we have to deal with scalability\n",
    "    \n",
    "    \n",
    "<b>2. N-grams</b>\n",
    "    \n",
    "To counter the issue of scalability, NLP (natural language processing) researchers introduced the idea of N-grams, where you take into account an ‘n’ number of words for conditional and joint probability. For instance, if n is equal 2, then only the previous two words of the sentence will be used to calculate joint probability instead of the entire sentence. \n",
    "    \n",
    "This approach reduces the scalability issue, but not completely. \n",
    "\n",
    "The disadvantages of N-grams are:\n",
    "\n",
    "1. Context of the sentence is lost if the sentence is long.\n",
    "2. Reduces the scalability issue by a small scale\n",
    "    \n",
    "    \n",
    "    \n",
    "<b> 3. Context Vectorizing</b>\n",
    "Context vectorizing is an approach where the input sequence is summarized to a vector such that that vector is then used to predict what the next word could be. \n",
    "    \n",
    "![contextvect](./Context-vectorizing.jpg)\n",
    "    \n",
    "The advantages of context vectorizing are:\n",
    "\n",
    "1. Order is preserved\n",
    "2. Can operate in variable length of sequences\n",
    "3. It can learn hence differentiable (backpropagation)\n",
    "4. Context is preserved in short sentences or sequences. \n",
    "    \n",
    "\n",
    "1. Context vectoring acts as “memory” which captures information about what has been calculated so far, and enables RNNs to remember past information, where they’re able to preserve information of long and variable sequences.\n",
    "2. Because of that, RNNs can take one or multiple input vectors and produce one or multiple output vectors. \n",
    "\n",
    "    \n",
    "***Which layer is used as contect vector***\n",
    "    \n",
    "The hidden state h(t) represents a contextual vector at time t and acts as “memory” of the network.\n",
    "    \n",
    "We denote a hidden state using this formula:\n",
    "\n",
    "<b>ht= tanh(Whht-1 + Wxxt)</b>\n",
    "\n",
    "<b>When t = 1,</b>\n",
    "\n",
    "<b>h1= tanh(Whh0 + Wxx1), where x1 is ‘Cryptocurrency’, and h0 is initialised as zero</b>\n",
    "\n",
    "<b>When t = 2,</b>\n",
    "\n",
    "<b>h2= tanh(Whh1 + Wxx2), where x1 is ‘is’.</b>\n",
    "\n",
    "<b>When t = 3,</b>\n",
    "\n",
    "<b>h3= tanh(Whh2 + Wxx3), where x2 is ‘the’.</b> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propogation in RNN and Problems\n",
    "\n",
    "\n",
    "## Backpropogation\n",
    "Backpropation in RNN we travel from right to left minimizing loss using gradient through time. As Paramters are shareable so unfolding is used for Back propogation through Time(BPTT) \n",
    "\n",
    "Loss function used is Cross Entropy \n",
    "\n",
    "![unfold](./Unfolded-recurrent-network.jpg)\n",
    "\n",
    "![bptt](./RNN_backpropation_equation.jpg)\n",
    "\n",
    "\n",
    "## Issues\n",
    "\n",
    "<b>1. Vanishing Gradients: </b>\n",
    "\n",
    "\n",
    "<b>When the differentiating vector goes to zero exponentially fast, which in turn makes it difficult for the network to learn some long period dependencies, the problem is vanishing gradient.</b>\n",
    "\n",
    "    Coming to backpropagation in RNNs, \n",
    "    1. we saw that every single neuron in the network participated in the calculation of the output with respect to the            cost function. \n",
    "    2. Because of that, we have to make sure that the parameters are updated for every neuron to minimize the error, and          this goes back to all the neurons in time. \n",
    "    3. So, you have to propagate all the way back through time to these neurons.\n",
    "\n",
    "\n",
    "    Contextual vector\n",
    "\n",
    "    1. We also know that the contextual vector, or the hidden state parameter, is shared across the network to preserve            order and continuity. \n",
    "\n",
    "    2. During initialization, the parameter is assigned with a random number which is close to zero, and when the hidden          state moves forward in time it \n",
    "\n",
    "    3. gets multiplied by itself over at different time steps, making the gradient Wh smaller and smaller, essentially zero        to a point where it vanishes. \n",
    "\n",
    "    4. The lower the gradient is, the harder it is for the network to update the weights, and if the gradient is zero, the weights will not be updated.\n",
    "\n",
    "\n",
    "\n",
    "<b>2. Exploding gradients</b>\n",
    "\n",
    "    Exploding gradients occur when large gradients accumulate due to an unstable process, and result in very large updates to the parameters.\n",
    "\n",
    "    1. In RNNs, exploding gradients can occur during backpropagation and result in very large gradients essentially making large updates to the network parameters. \n",
    "\n",
    "    2. At an extreme, the values of weights can become so large that they become NaN values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overcoming gradient issues\n",
    "\n",
    "Gradient issues in RNNs can be solved with:\n",
    "\n",
    "1. Gradient clipping\n",
    "\n",
    "2. Gated networks \n",
    "\n",
    "\n",
    "<b>Gradient clipping</b>\n",
    "\n",
    "Gradient clipping is a technique used to avoid exploding gradients. It’s fair to assume that RNNs behave in an approximate linear fashion, which makes the gradient unstable. \n",
    "\n",
    "In order to control the gradient, it’s clipped, or reshaped to a smaller value. There are two ways to clip gradients:\n",
    "\n",
    "Clip the gradient from a mini batch just before the parameter is updated \n",
    "Use a hyperparameter C which measures the norm ||g|| where g is the gradient. If ||g|| > C then gg.C/||g|| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "This particular kind of RNN adds a forget mechanism, as the LSTM unit is divided into cells.\n",
    "\n",
    "Each cell takes three inputs: :\n",
    "\n",
    "1. current input, \n",
    "2. hidden state, \n",
    "3. memory state of the previous step (6). \n",
    "\n",
    "\n",
    "These inputs go through gates: \n",
    "\n",
    "1. input gate, \n",
    "2. forget gate, \n",
    "3. output gate. \n",
    "\n",
    "\n",
    "Benefits\n",
    "\n",
    "1. LSTM was able to overcome vanishing and exploding gradients in the RNN model,\n",
    "\n",
    "Problems\n",
    "\n",
    "1. No parallelization, we still have a sequential path for the data, even more complicated than before.\n",
    "2. Hardware resources are still a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder sequence-to-sequence architecture\n",
    "The advantage of using RNNs in sequential modeling is that it can:\n",
    "\n",
    "Map an input sequence to a fixed-size vector\n",
    "Map fixed-size vector to a sequence\n",
    "Map an input sequence to an output sequence of the same length. \n",
    "But let’s say we want to train a RNN to map an input sequence to an output sequence, not necessarily of the same length. This can come up especially when we want to translate from one language to another. \n",
    "\n",
    "Encoder-decoder sequence-to-sequence is an architecture that deals with this type of problem. As the name suggests, it has two types of architecture: encoder and decoder. \n",
    "\n",
    "Encoder RNN receives the input sequence of variable length, and processes it to return a vector or a sequence of vectors called the “context” vector C.\n",
    "\n",
    "The decoder RNN is conditioned on a fixed-length vector to generate an output sequence. Also, the last hidden state of the encoder is the initial hidden state of the decoder. \n",
    "\n",
    "![encoderdecoder](./Encoder-decoder.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference :\n",
    "\n",
    "Explanation and code    \n",
    "\n",
    "https://neptune.ai/blog/recurrent-neural-network-guide\n",
    "\n",
    "https://www.tensorflow.org/guide/keras/rnn\n",
    "\n",
    "https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\n",
    "\n",
    "https://towardsdatascience.com/how-to-use-torchtext-for-neural-machine-translation-plus-hack-to-make-it-5x-faster-77f3884d95\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Begin Transformers\n",
    "\n",
    "Transformer model is designed consisting of two main components\n",
    "\n",
    "1. Encoder\n",
    "\n",
    "2. Decoder \n",
    "\n",
    "Encoder\n",
    "\n",
    "composed of a stack of multiple identical layers, each layer containing two sublayers, multi-headed self-attention mechanism followed by residual connections, and simple-wise fully connected feed-forward network. \n",
    "\n",
    "1. The input will be the word embeddings for the first layer. For subsequent layers, it will be the output of previous layer.\n",
    "\n",
    "2. Inside each layer, first the multi-head self attention is computed using the inputs for the layer as keys, queries and values.\n",
    "\n",
    "3. The output of #2 is sent to a feed-forward network layer.\n",
    "\n",
    "4. Here every position (i.e. every word representation) is fed through the same feed-forward that contains two linear transformations followed by a ReLU (input vector ->linear transformed hidden1->linear transformed hidden2 ->ReLU output).\n",
    "\n",
    "\n",
    "Decoder \n",
    "\n",
    "1. The input will be the word embeddings generated so far for the first layer. For subsequent layers, it will be the output of previous layer.\n",
    "\n",
    "2. Inside each layer, first the multi-head self attention is computed using the inputs for the layer as keys, queries and values (i.e. generated decoder outputs so far, padded for rest of positions).\n",
    "\n",
    "3. The output of #2 is sent to a “multi-head-encoder-decoder-attention” layer. Here yet another attention is computed using #2 outputs as queries and encoder outputs as keys and values.\n",
    "\n",
    "4. The output of #3 is sent to a position wise feed-forward network layer like in encoder.\n",
    "\n",
    "\n",
    "\n",
    "### Attention\n",
    "This model was inspired by the human vision system (7). As a brain receives a massive input of information from the eyes, more than the brain can process at a time, the attention cues in the eye sensory system make humans capable of paying attention to a fraction of what the eyes receive.\n",
    "\n",
    "\n",
    "![Attention](self_attention.jpg)\n",
    "The Transformer model uses a “Scaled Dot Product” attention mechanism.\n",
    "\n",
    "We can apply this methodology to the problem at hand. If we know the parts that can affect our translation, we can focus on those parts and ignore the other useless information. \n",
    "\n",
    "This will affect the system’s performance. While you’re reading this article, you’re paying attention to this article and ignoring the rest of the world. This comes with a cost that can be described as the opportunity cost. \n",
    "\n",
    "We can select from different types of attention mechanisms, like attention pooling and fully-connected layers.\n",
    "\n",
    "In attention pooling, inputs to the attention system can be divided into three types:\n",
    "\n",
    "the Keys (the nonvolitional cues),\n",
    "\n",
    "the Queries (Volitional Cues), \n",
    "\n",
    "the Values (the sensory inputs). \n",
    "\n",
    "\n",
    "\n",
    "### Different Types of Attention\n",
    "\n",
    "![attention_types](./attention_types.jpg)\n",
    "\n",
    "\n",
    "### Self Attention\n",
    "\n",
    "\n",
    "<b>High Level Understanding</b>\n",
    "\n",
    "Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.\n",
    "for better explore [notebook](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)\n",
    "\n",
    "\n",
    "<span style=\"color: orange;\">\"The animal didn't cross the street because it was too tired”</span>\n",
    "\n",
    "\n",
    "What does <span style=\"color: orange;\">“it”</span> in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.\n",
    "\n",
    "When the model is processing the word <span style=\"color: orange;\">“it”</span> , self-attention allows it to associate “it” with “animal”.\n",
    "\n",
    "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
    "\n",
    "\n",
    "\n",
    "<b>In Detail</b>\n",
    "\n",
    "<b>Steps</b>\n",
    "  1. <b>Calculate three vectors</b> from encoder input vector i.e. embedding of each word\n",
    " \n",
    "    1. Query \n",
    "    2. Key\n",
    "    3. Value\n",
    "\n",
    "Note: Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512\n",
    "\n",
    "  2. The second step in calculating self-attention is to <b>calculate a score</b>. Say we’re calculating the self-attention for the first word in this example, <span style=\"color: orange;\">“Thinking”</span>. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.\n",
    "\n",
    "    i. The score is calculated by taking the <b>dot product of the query vector with the key vector</b> of the respective word we’re scoring. \n",
    "\n",
    "    ii. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.  \n",
    "\n",
    "\n",
    "\n",
    "  3. The third and fourth steps are to <b>divide the scores by 8 </b>(the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients.\n",
    "  \n",
    "  \n",
    "  4. There could be other possible values here, but this is the default), then pass the result through a <b>softmax operation.</b> Softmax normalizes the scores so they’re all positive and add up to 1\n",
    "  \n",
    "\n",
    "  5. Multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n",
    "  \n",
    "\n",
    " 6. sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n",
    " \n",
    "\n",
    "  \n",
    "![selfattention](./self-attention-output.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\Pytorchgpu\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\HP\\anaconda3\\envs\\Pytorchgpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\HP\\anaconda3\\envs\\Pytorchgpu\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\\n%s\" %\n"
     ]
    }
   ],
   "source": [
    "# http://nlp.seas.harvard.edu/2018/04/03/attention.html#background\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = \\\n",
    "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (query, key, value))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x, self.attn = attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous() \\\n",
    "             .view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-6.5165e-01, -4.8202e-01,  1.8513e+00,  ..., -3.5028e-03,\n",
      "           2.4477e-01, -7.9970e-01],\n",
      "         [-1.3169e+00, -7.8400e-02,  1.5168e+00,  ...,  4.3083e-01,\n",
      "           4.2076e-01, -2.2733e+00],\n",
      "         [-5.0611e-02,  2.4816e-01,  2.1436e+00,  ..., -6.5838e-01,\n",
      "           1.8618e-01, -7.8179e-01],\n",
      "         ...,\n",
      "         [-3.3425e-01, -3.8498e-03,  2.3675e+00,  ...,  1.0746e-01,\n",
      "           6.0675e-01, -2.1961e+00],\n",
      "         [-1.7190e-01, -5.2445e-02,  1.9160e+00,  ...,  8.2587e-01,\n",
      "          -4.9010e-01, -2.2013e+00],\n",
      "         [-6.2436e-03, -3.1832e-01,  1.3241e+00,  ..., -1.3040e+00,\n",
      "           6.0892e-01, -1.6055e+00]],\n",
      "\n",
      "        [[ 1.9632e-01,  3.2807e-01,  1.6494e+00,  ..., -3.6864e-01,\n",
      "          -3.6389e-01, -6.3401e-01],\n",
      "         [-6.6213e-01,  6.9574e-03,  1.0705e+00,  ..., -3.9800e-01,\n",
      "           3.7914e-01, -1.2937e+00],\n",
      "         [ 3.7505e-02, -2.6562e-01,  1.3161e+00,  ..., -8.5314e-01,\n",
      "           4.6169e-01, -1.8740e+00],\n",
      "         ...,\n",
      "         [ 2.1817e-01, -6.7582e-02,  1.5832e+00,  ..., -7.1819e-01,\n",
      "           8.9997e-01, -1.3609e+00],\n",
      "         [-9.1497e-02, -1.7617e-01,  7.8291e-01,  ..., -1.5312e+00,\n",
      "           2.7267e-01, -2.2399e+00],\n",
      "         [-1.5494e-01, -7.0475e-01,  1.6155e+00,  ..., -7.1193e-01,\n",
      "           2.1885e-01, -2.3123e+00]],\n",
      "\n",
      "        [[-6.3701e-02,  3.2355e-02,  1.0015e+00,  ..., -5.5731e-01,\n",
      "           1.0079e+00, -1.8131e+00],\n",
      "         [-5.0794e-01,  1.0450e-01,  1.9956e+00,  ...,  1.9773e-02,\n",
      "          -7.0793e-01, -1.7294e+00],\n",
      "         [-7.2883e-02, -6.8991e-01,  2.2477e+00,  ...,  5.3425e-01,\n",
      "           5.0611e-01, -1.1208e+00],\n",
      "         ...,\n",
      "         [-2.3896e-01,  2.7399e-01,  1.9277e+00,  ...,  1.2576e-01,\n",
      "           1.7408e-01, -2.7316e+00],\n",
      "         [-1.4411e-01, -3.1424e-01,  1.8967e+00,  ...,  5.9340e-01,\n",
      "           3.2103e-01, -3.2207e+00],\n",
      "         [ 2.2483e-01, -3.4076e-01,  1.5092e+00,  ..., -3.3796e-01,\n",
      "           4.1582e-01, -7.6252e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 3.7586e-01,  2.5664e-01,  1.5163e+00,  ..., -3.7529e-01,\n",
      "           5.0741e-01, -1.5734e+00],\n",
      "         [-3.7300e-01,  8.7499e-01,  1.5339e+00,  ...,  9.7958e-02,\n",
      "           5.8455e-01, -7.8550e-01],\n",
      "         [ 1.2401e-02,  2.2610e-01,  1.5457e+00,  ..., -9.9356e-01,\n",
      "          -6.2343e-01, -1.5211e+00],\n",
      "         ...,\n",
      "         [ 3.5026e-01,  1.4807e-01,  2.4687e+00,  ..., -5.3459e-01,\n",
      "          -6.7442e-02, -2.0458e+00],\n",
      "         [-3.2943e-02, -2.4054e-01,  1.3137e+00,  ..., -5.4393e-01,\n",
      "           2.1455e-02, -2.1625e+00],\n",
      "         [-3.9481e-02, -1.3374e+00,  1.1790e+00,  ..., -2.1357e-01,\n",
      "          -3.6663e-01, -1.1809e+00]],\n",
      "\n",
      "        [[-7.7752e-02, -5.0767e-01,  1.7002e+00,  ..., -7.2391e-01,\n",
      "           1.1956e+00, -1.8744e+00],\n",
      "         [-7.4046e-01,  4.1811e-01,  1.8252e+00,  ..., -1.7857e-01,\n",
      "          -6.7334e-01, -1.4897e+00],\n",
      "         [-1.5169e-01,  4.0600e-02,  2.0279e+00,  ...,  4.9083e-01,\n",
      "           2.9043e-01, -1.9135e+00],\n",
      "         ...,\n",
      "         [ 2.8931e-01, -2.4781e-03,  2.2271e+00,  ..., -1.5862e-01,\n",
      "           3.5261e-01, -2.0371e+00],\n",
      "         [-5.2027e-01, -4.2120e-01,  2.1609e+00,  ..., -4.6582e-01,\n",
      "           1.8567e-01, -1.9747e+00],\n",
      "         [-2.1121e-01, -5.6250e-01,  1.7085e+00,  ...,  1.9234e-02,\n",
      "           8.0582e-01, -1.8782e+00]],\n",
      "\n",
      "        [[-1.4138e-01, -6.6397e-01,  1.8681e+00,  ...,  1.2031e-01,\n",
      "           1.1390e+00, -1.7923e+00],\n",
      "         [-5.6340e-01,  3.4830e-01,  1.8516e+00,  ..., -5.5144e-01,\n",
      "          -1.8251e-01, -1.2464e+00],\n",
      "         [ 7.2588e-01,  2.3156e-01,  1.8901e+00,  ...,  9.3457e-02,\n",
      "           5.0382e-01, -2.3432e+00],\n",
      "         ...,\n",
      "         [-5.3805e-02, -2.5546e-01,  1.8531e+00,  ..., -6.0924e-01,\n",
      "          -1.9327e-01, -1.4800e+00],\n",
      "         [-3.0129e-01, -9.0662e-01,  1.3673e+00,  ..., -5.7913e-01,\n",
      "          -3.6658e-01, -1.2785e+00],\n",
      "         [ 4.4371e-01, -9.7053e-01,  2.1979e-01,  ...,  4.5907e-01,\n",
      "          -2.4120e-02, -6.1620e-01]]], grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    ">>> src = torch.rand((10, 32, 512))\n",
    ">>> tgt = torch.rand((20, 32, 512))\n",
    ">>> out = transformer_model(src, tgt)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query and type Linear(in_features=512, out_features=512, bias=True) <class 'torch.nn.modules.linear.Linear'>\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "Embedding(3, 3)\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 4]])\n",
      "tensor([4, 5, 4])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "d_model=512\n",
    "\n",
    "q_linear = nn.Linear(d_model, d_model)\n",
    "print('query and type',q_linear,type(q_linear))\n",
    "\n",
    "key_linear = nn.Linear(d_model, d_model)\n",
    "print(key_linear)\n",
    "\n",
    "a=[[1,2,3],[4,5,4]]\n",
    "emb=nn.Embedding(3,3)\n",
    "print(emb)\n",
    "\n",
    "key=torch.tensor(data=a)\n",
    "print(key)\n",
    "print(key[-1])\n",
    "key_transpose=key.transpose(-2, -1)\n",
    "print(key_transpose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://neptune.ai/blog/comprehensive-guide-to-transformers\n",
    "\n",
    "https://towardsdatascience.com/self-attention-and-transformers-882e9de5edda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=GSt00_-0ncQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BERT \n",
    "\n",
    "SEntence BERT\n",
    "https://www.pinecone.io/learn/sentence-embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sent_list=[i for i in sent.split()]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
